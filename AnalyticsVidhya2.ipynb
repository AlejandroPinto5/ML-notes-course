{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDBEDURTIEa19K6vbe6g70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejandroPinto5/ML-notes-course/blob/main/AnalyticsVidhya2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Machine Learning Lifecycle**"
      ],
      "metadata": {
        "id": "GS2-SKY5fh2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Six steps pf predictive model:\n",
        "\n",
        "                                - Problem definition\n",
        "                                - Hypothesis Generation\n",
        "                                - Data extraction / Collection\n",
        "                                - Data Exploration and transformation\n",
        "                                - Predictive model\n",
        "                                - Model Deployment / Implementation\n",
        "\n",
        "\n",
        "Predictive modeling is a process to create statistical model for estimating/predicting the future behaviour based on past data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pmy2lCpHfo0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importance of Stats and EDA**\n"
      ],
      "metadata": {
        "id": "jy5OkFN8frEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E**xploratory **D**ata **A**nalysis\n",
        "Descriptive Statistics:\n",
        "\n",
        "                        Summarize data\n",
        "                        Visual representation as histogram, bar and pie plots\n",
        "                        Central tendency\n",
        "                        Relation variables\n",
        "                  \n",
        "Inferencial Statistics:\n",
        "\n",
        "                        Aproximation of Data\n",
        "                        Try to validate a hypothesis\n",
        "                        Error analysis of sample\n",
        "\n"
      ],
      "metadata": {
        "id": "RK9KEIHrftnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Build your first predictive model**\n"
      ],
      "metadata": {
        "id": "uDIdRPUmf9vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps for benchmark models**:\n",
        "\n",
        "      Hypothesis generation:\n",
        "      Get dataset ready for your models: Divide the data in training (build and train model) set and testing set\n",
        "      Modelling process: Supervised ML (Regression or classification)\n",
        "      Evaluation:\n",
        "\n",
        "\n",
        "When you have a dataset, add the data to your code: \n",
        "\n",
        "```\n",
        "df = pd.read_csv('')\n",
        "```\n",
        "the see if you have a missed values:\n",
        "\n",
        "\n",
        "```\n",
        "data.ismull().sum()\n",
        "```\n",
        "\n",
        "Then you create train and test from your dataset. You can divide your dataet with pandas and scikitlearn:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.util import shuffle\n",
        "df = shuffle(df, random_state = 42)\n",
        "div = int(df.shape[0]/4)\n",
        "\n",
        "#Three parts of training set and one part of testing set\n",
        "train = df.loc[:3*div+1,:]\n",
        "test = df.loc[3*div+1:]\n",
        "```\n",
        "\n",
        "\n",
        "From here, you can create your models.\n",
        "\n",
        "****"
      ],
      "metadata": {
        "id": "rUE8YHXPgAua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e.g. is a simple mean model for regression model**: First you store any variable's mean of the train set in a new column of test set:\n",
        "\n",
        "```\n",
        "test['simple_mean'] = train['item_Outlet_sales'].mean()\n",
        "```\n",
        "And calculate errors (evaluation metric for regressions - mae):\n",
        "\n",
        "```\n",
        "From sklearn.metrics import mean_absolute_error as mae\n",
        "simple_mean_error = mae(test['Item_Outlet_sales'], test['simple_mean'])\n",
        "simple_mean_error\n",
        "```\n",
        "Doing smater predictions, we can take mean of any other value, like outlet_type, Outlet_location_type or/and Outlet_establisment_year:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "out_type = pd.pivot_table(train, values = 'Item_outlet_sales', index = ['Outlet_type'], aggfunc = np.mean)\n",
        "out_type\n",
        "```\n",
        "\n",
        "Create the model(with **for** to running a loop for any mean calculated), and calculate the error. You reduce error\n",
        "\n",
        "****\n"
      ],
      "metadata": {
        "id": "IRVhvdTDgJ1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e.g. is a simple mode model for classification model**: First you store any variable's mode of the train set in a new column of test set:\n",
        "\n",
        "```\n",
        "test['simple_mode']= train['survived'].mode()[0]\n",
        "```\n",
        "\n",
        "then, you calculate the accuracy (evaluation metric for classification):\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "simple_mode_accuracy = accuracy_score(test['survived'], test['simple_mode'])\n",
        "simple_mode_accuracy\n",
        "```\n"
      ],
      "metadata": {
        "id": "ruPKB-t5gPmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluation metrics**"
      ],
      "metadata": {
        "id": "2byOx5cOgT9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix** is for clasification problems. Represent preficted values against actual values. True positive(TP), False negative(FN), False positive(FP) and True negative(TN).\n",
        "\n",
        "**Accuracy** is for classification tasks. Accuracy = (Correct predictions)/(Total predictions) = (TP + TN)/(TP + TN + FP + FN)\n",
        "\n",
        "**True positive rate** is alternative to accuracy. TPR = TP/(TP + FN). High value, better model.\n",
        "\n",
        "**False negative rate** is alternative to accuracy. FNR = FN/(TP + FN).Low value, better model.\n",
        "\n",
        "**True negative rate** is alternative to accuracy. TNR = TN/(FP + TN). High value, better model.\n",
        "\n",
        "**False positive rate** is alternative to accuracy. FPR = FP/(FP + TN). Tend to zero, better model.\n",
        "\n",
        "How to select a metric?\n",
        "\n",
        "                        Use a combination of both\n",
        "                        Validation\n",
        "                        Depends on use case\n",
        "\n",
        "\n",
        "**Precision** Predictions Actually Positive/ Total predicted positive. Presicion = TP/(TP + FP). Minimize FP\n",
        "\n",
        "**Recall** Predictiones Actually positive / Total Actual Positive. Recall = TP / (TP + FN). Minimize FN\n",
        "\n",
        "F1 is maximum when Presicion = recall. F1 = 2 /((1/presicion) + (1/recall))\n",
        "\n",
        "**Thresholding** don't get it\n",
        "\n",
        "**AUC-ROC**. Probability of classes. It's plotted between TPR and FPR. TPR should be greater than FPR. don't get it\n",
        "\n",
        "**Log loss** Probability of classes. Compare two models. Considers order of probability. It's the negative average of the log of corrected predicted probabilities for each instance. Corrected probabilities mean. The lower log loss, the better predictions.\n",
        "\n",
        "**Evaluation Metrics for Regression** \n",
        "\n",
        "                                    MAE: \n",
        "                                    MSE:\n",
        "                                    RMSE:\n",
        "                                    RMSLE:\n",
        "                                    R-Squared: 1-MSE(model)/MSE(baseline: mean values). higher value, better model.\n",
        "                                    Adjusted R-Squared: Include numnber of samples and number of features.\n"
      ],
      "metadata": {
        "id": "Mnh3KI6IgWyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Processing Data**\n"
      ],
      "metadata": {
        "id": "_uS7ES4Xgax7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Input missing values\n",
        "                    1. Deleting (not recommended)\n",
        "                          data.isnull().sum()\n",
        "                          data_row_del = data.dropna(axis =0)\n",
        "                          data_del_col = data.dropna(thresh=500, axis =1)\n",
        "\n",
        "                    2. Replacing (isn't the best method)\n",
        "                          data['column'].fillna(value = 'missing')\n",
        "                          (data['column'].isnull()).astype('int') --remplazing with 1 and 0s\n",
        "\n",
        "                    3. Inputing\n",
        "                          Central tendency\n",
        "                              data['col'].mean() or data['col'].mode()\n",
        "                              data['column'].fillna(value = mean)\n",
        "\n",
        "                          Relationship with features\n",
        "                              data.corr()\n",
        "                              \n",
        "                          ML model (this is the most popular)\n",
        "\n",
        "2.   Remove Categorical (string) values\n",
        "                          There are two types: \n",
        "                          1. Nominal (gender, size)\n",
        "                              data['col'].nunique()\n",
        "                              pd.get_dummies(data['col']).head()\n",
        "\n",
        "                          2. Ordinal (names, education)\n",
        "\n",
        "3.   Treat Outliers. Outliers are extreme values relatives to other observations.\n",
        "\n",
        "                               IQR  = data.['col'].qunatile(0.75) -data.['col'].qunatile(0.75)\n",
        "                               Q1 = data.['col'].qunatile(0.25)\n",
        "                               Q2 = data.['col'].qunatile(0.75)\n",
        "\n",
        "                               whisker_1 = Q1-(1.5*IQR)\n",
        "                               whisker_2 = Q3+(1.5*IQR)\n",
        "\n",
        "                          1. Delete the data points (not advice)\n",
        "                               data_new = data.loc[data['col'] < whisker_2\n",
        "\n",
        "                          2. Replace outliers (central tendency, replace values or whisker values, using ML model)\n",
        "                               data.loc[data['col'] < 7 =Q1\n",
        "\n",
        "                          3. Transform the values or binning values\n",
        "                               (data['col']).hist()\n",
        "                               (np.sqrt(data['col'])).hist()\n",
        "                               (np.log(data['col'])).hist()\n",
        "\n",
        "\n",
        "4.   Feature scaling and variable transformation\n"
      ],
      "metadata": {
        "id": "AN3_1J4tgdoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Build your fisrt ML model**"
      ],
      "metadata": {
        "id": "QqygeL0hglsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**: Observe nature of nearest neighbours, lazy learning algorithm and simplest ML algorithm.\n",
        "\n",
        "Steps:\n",
        "\n",
        "    1. Plot the training dataset.\n",
        "    2. Locate the new 'test' instance.\n",
        "    3. Calculate distance from all train data points.\n",
        "    4. Sort the distance list in ascending order.\n",
        "    5. Choose the first k distance from the sorted list.\n",
        "\n",
        "For classification model is mode, for regression model is mean.\n",
        "\n",
        "Determine the value of ***k*** is done by elbow method, you choose the minimun error (k-value vs error). Like whats the best energy used for that problem.\n",
        "\n",
        "How to calculate distance:\n",
        "      \n",
        "                          D = (sum (pi-qi)^k)^1/k\n",
        "\n",
        "    1. Manhattan distance: Sum of absolute differences between two points.\n",
        "        D = sum (pi-qi)\n",
        "\n",
        "    2. Euclideandistance: Shortest distance between two points.\n",
        "        D = Sqrt(sum (pi-qi)^2)\n",
        "\n",
        "    3. Minkowiski distance\n",
        "        D = (sum (pi-qi)^p)^1/p\n",
        "\n",
        "    4. Hamming distance (for categorical values): Total number of differences between two strings of identical length. You count as distance the change value.\n",
        "\n",
        "Fails when values have different units (distance vs price). The problem is resolved normalizing data.\n",
        "\n",
        "For sklearn:\n",
        "\n",
        "    1. Preprocessing:\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    2. Model Selection.\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "    3. Building a model.\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "    \n",
        "    4. Model Evaluation.\n",
        "        from sklearn.metrics import accuracy_score \n",
        "\n",
        "Scale\n",
        "\n",
        "Split\n",
        "\n",
        "Choose k \n",
        "\n",
        "Implement \n",
        "\n",
        "Evaluate"
      ],
      "metadata": {
        "id": "wNBVttjmgpq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Selecting the Right Model**"
      ],
      "metadata": {
        "id": "JlPwZQUzgueC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting and Underfitting**\n",
        "For knn model, in the plot of wlbow method where we compared train and test fi score, there is a region between 21 and 60 wherevalidation and train score are low (underfitting), zone that the  model is no learning.  The zone between 0 and 12 is overfitting trying to memorize data.  The bets fit region is between 12 and 21.\n",
        "\n",
        "**High bias** meansthat the model is not able to learn avaliable signals or patterns in the data. High bias, low variance, low complexity on the model (underfitted). Low bias, high variance, high complexity (overfitted). See bias and variance plot (error vs. complexity)\n"
      ],
      "metadata": {
        "id": "2u2ZSsSAg45K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation set:**\n",
        "Its between train set and test set. we do this to make sure the model does not overfit on the test data. Validation is used for optimization of model and test set for the final evaluation. It's created from train set.\n",
        "\n",
        "    1. Hold-out Validation. First the dataset is splitted into two parts, train and test usinftrain_test_split() function. Then the train set is splitted into two parts using the same function.\n",
        "\n",
        "    2. Stratified hold-out Validation. When adataset is unbalanced, you can use stratify parameter to split the dataset with the same proportions of classes as in the target variable.\n",
        "    \n",
        "    3. Leave one out. N instance N models, very computational expensive. It's practice for small dataset.\n",
        "\n",
        "    4. k-fold cross validation. For setting the value of k, cross_val_score function with parameter cv = k is used. For 5-fold cross validation set cv = 5 along with ohter parameter. You don't need to split train and validation set explicitly:\n",
        "       \n",
        "        1. Shuffle the dataset randomly\n",
        "        2. Split the dataset into k groups:\n",
        "            Pick a group as a hold out.\n",
        "            Take the remaining groups as training and fit a model.\n",
        "            Predict and evaluate on the hold out.\n",
        "        3. Repeat the above procedure with every group."
      ],
      "metadata": {
        "id": "xeisKlPihQEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Linear Models**"
      ],
      "metadata": {
        "id": "WkTv5ZhBhWmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use linear models when there are a corelation between dependent and independent variables. Show a linear pattern. Low MSE = better model.\n",
        "\n",
        "```\n",
        "y = ax + b\n",
        "```\n",
        "**Cost Function:** The cost function of a linear regression is mean squared error. If we have a data, we can create a predicted line:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "a = parameter\n",
        "b = parameter\n",
        "line = []\n",
        "for i in range(len(data)):\n",
        "  line.append(a*data.x[i] + b)\n",
        "```\n",
        "We can use mse to calculate error:\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "MSE = mse(data.y, line)\n",
        "```\n",
        "Remember: Low MSE = better model. But, How to know the best parameter?. So, for ***a*** parameter:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#Range of slopes from 0 to 1.5 with increment of 0.01\n",
        "slope = [i/100 for i in range(0,150)]\n",
        "cost = []\n",
        "for i in slope:\n",
        "    costtmp = Error(a = i, data = data)\n",
        "    cost.append(costtmp)\n",
        "\n",
        "```\n",
        "\n",
        "And Error is a function:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def Error(a, data)\n",
        "  b = 1.1 # still parameter\n",
        "  y = []\n",
        "  x = data.x\n",
        "  for i in range(len(data.x)):\n",
        "      tmp = a*data.x[i] + b\n",
        "      y.append(tmp)\n",
        "  MSE = mse(data.y, y)\n",
        "  return y\n",
        "```\n",
        "Plot a(slope) vs cost\n",
        "\n",
        "\n",
        "Gradient descent is a supervised ML tecnique.\n",
        "\n",
        "If there isn't a linear pattern, you can use tramsformation methods to find a pettern (log(x), rootX, squared x). Linear models assume that there isn't correlation between error terms. \n",
        "\n",
        "**Issues with Linear Regression:** \n",
        "    1. More features are participating, leads to more coefficients.\n",
        "    2. Values of coefficinets are large.\n",
        "    3. Overfitting, performing well on train but not on test dataset.\n",
        "\n",
        "Regularization comes to help.\n",
        "\n",
        "***Ridge regularization***, sum a penalty of parameter wich is square of all coeficients plus mean of squared error. We obtain a linear regression cost function. Nullifiedcoefficients still participate. Create a difficulty in interpretation.\n"
      ],
      "metadata": {
        "id": "6UNZXqo4hanQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree**"
      ],
      "metadata": {
        "id": "kx-rSYWshq4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective of desicion tree is to have pure nodes. am ideal pure node hace  100% of one class and 0% of other classes.\n",
        "\n",
        " The basic idea behind choosing the best split in the data is resultant nodes should be as homogeneous as possible.\n",
        "\n",
        "We can split and split and so on, into multiple desicion.\n",
        "\n",
        "**Common Terminology:**\n",
        "\n",
        "    1. Root node: Representsthe entire population of data.\n",
        "    2. Splitting: Process to divide a node into 2 or more sub-nodes.\n",
        "    3. Decision node: The inicial node.\n",
        "    4. Leaf/Terminal node: The final node, Don't split further.\n",
        "    5. Branch: Sub-tree.\n",
        "    6. Parent and child node: Higher node is the parent and subnodes are children.\n",
        "    7. Depth of tree: The length of the longest path for the root node.\n",
        "\n",
        "\n",
        "**Best split point in Desicion Trees**\n",
        "\n",
        "1. Decision tree splits the nodes on all available variables.\n",
        "2. Selects the split which results in most homogeneous sub-nodes.\n",
        "\n",
        "    1. **Gini impurity:** 1-Gini: If you pick to points from a popular random, they must be from the same class. Lower the gini impurity, higher the homogenety of nodes. Works only with categorical targets.\n",
        "\n",
        "              gini = (p1**2+p2**2+p3**2+...)\n",
        "    \n",
        "    2. **Chi Squared:** Statistical significance of differences between child nodes and their parent node. Higher chi-squared value, more will be the purity of the nodes after split. works only with categorical values. Can perfomr more than two splits.  \n",
        "\n",
        "              chi-squared = (Actual- Expected)**2/ Expected frequencies\n",
        "    \n",
        "    3. **Information Gain:** 1 - entropy: If entroy is zero, represents pure node. High information gain means higher the homogeneity of nodes after the split.\n",
        "\n",
        "              Entropy = -p1*log2p1-p2*log2p2-p3*log2p3-...pn*log2pn  \n",
        "\n",
        "    4. **Reduction in Variance:** Uses standard formula of variance. If variance zero, pure node. Used when target is ***continuous***. Split with lower variance is selected.\n",
        "\n",
        "              Variance = sum[(x-mu)**2]/n\n",
        "              X is the sample\n",
        "              mu is mean\n",
        "              n is number of samples\n",
        "\n",
        "3. Optimizing performance of Decision Tree: Higher values controls overfitting. Too high value can lead to underfitting.\n",
        "      \n",
        "      1. Set minimum samples for a node split.\n",
        "      2. Set minumum samples for a terminal node.\n",
        "      3. Set maximum depth of tree.\n",
        "      4. set maximum terminal nodes.\n",
        "\n",
        "\n",
        "\n",
        "What is the maximum number of terminal nodes in a decision tree if our training dataset has N samples? N. Linear regression is parametetric, decision tree is not parametric.\n"
      ],
      "metadata": {
        "id": "uuWIMiUuhsyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Enginieering**"
      ],
      "metadata": {
        "id": "pk_QZjUahwuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering is the process of extracting information from the existing data in order to improve the performance of the model.\n",
        "\n",
        "There are two types of:\n",
        "  1. **Feature Preprocessing:** Updating or transforming existing features.\n",
        "      1. **Feature Transformation:** is used for transforming non-linear data into linear and also to reduce the skewness of the variable. It's a method used to transform features using maths operations (Log, Square, root, ...). Log transformation is used for removing right skewness from the data. You can do this with: \n",
        "\n",
        "              np.sqrt(df['column'])\n",
        "              np.log(df['column']+0.1) # 0.1 to eliminate error at 0.\n",
        "        \n",
        "      2. **Feature Scaling:** Two common ways to scale data: Min Max Scaler and Standard scaler. you can see if MinMax Scaler works with:\n",
        "\n",
        "              dfScaled.describe()\n",
        "\n",
        "        The Min and Max must be 0 and 1, respectively. Distance based algorithms like k-NN do get affected by the different scaling of the features, hence we always need to scale or normalize our data while working with these algorithms.\n",
        "\n",
        "      3. **Feauture One Hot Encoding:** ML doesn't know how to deal categorical values, so you change categorical values with binary values. You can do this with:\n",
        "\n",
        "              df.dtypes # to see object data type\n",
        "              df.['column'].value_counts # to see how many categories we have in a column\n",
        "              pd.get_dummies(df['column']).head() # to convert categorical values into binary values. \n",
        "        \n",
        "          You do this with all categorical columns. But, if you want, you can convert all columns at the same time.\n",
        "                \n",
        "                pd.get_dummies(df)\n",
        "                \n",
        "          We have some problems with this. Loss of information. You can solve this problem with ***LabelEncoder***:\n",
        "              \n",
        "                from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "          And transform ordinal to number variables, i.e. If we have 4 feature like 'small', 'medium' 'large' and 'extra-large'. Label Encoder transform into (0, 1, 2, 3). And solve loss information problem.  Nominal variables are differents. The other problem is number of features increase drastically, you can solve with Combine spare Class.\n",
        "        \n",
        "      4.  **Feature Combine Sparse Class:** Spare classes are classes with relative low frequency. We're gonna combine low frequency classes.\n",
        "                \n",
        "                df.unique()\n",
        "                temp = df['column'].value_counts()\n",
        "                df['column_count'] = df['column'].apply(lambda x: temp[x])\n",
        "                for i in range(0, len(df))\n",
        "                  if df['column_count'][i] < 4:\n",
        "                    df['column'][i] = 'other'\n",
        "              \n",
        "\n",
        "  2. **Feature Generation:** Creating new features.\n",
        "\n",
        "      1. **Feature Binning:** Binning or discretization is the process of transforming numerical variables into categorical. Separate variables in range.  \n",
        "\n",
        "              bins = [0,12,19,...(a range)]\n",
        "              pd.cut(df['column'], bins)\n",
        "      \n",
        "      2.  **Feature Interaction:** Creating columns with arithmetic operations. We can use any number of features or columns while performing feature interaction.\n",
        "\n",
        "      3. **Feature Missing Values:** Missing values might have a pattern.\n",
        "\n",
        "              df['column'].isnull()*1 # or what you need\n",
        "            \n",
        "      4. **Feature Frequiency Encoding:** Used for categorical values. We can determine frequency of categories in this variable and normalize the values.\n",
        "              temp = df['column'].value_counts()\n",
        "              df['column'].aply(lambda x: temp[x])\n",
        "              df.groupby('column')['other-column'].mean()\n",
        "        \n",
        "  3. **Date Time Feature:** Focused on date-time variable. Sometimes, we need to convert variables to datetime  tpye:\n",
        "\n",
        "         pd.to_datetime(df['column'], format = '%d/%m/%Y %H.%M.%S')\n",
        "  \n",
        "  You can see max and min values:\n",
        "        \n",
        "         df['column'].max(), df['column'].min()\n",
        "  \n",
        "  You can extractinfo:\n",
        "\n",
        "         df['column'].dt.[some-you-need].head() # Some you need can be month, dayofweek, minute, second, is_month_end, year, hour...\n",
        "\n",
        "  The to_datetime() function of pandas is used to convert string date-time into python date time object.\n",
        "\n",
        "  4. **Feature Tools:** Automated feature engineering. Its a librarie to create new features. It's really powerful. Feature tools can very automate the process of feature engineering but one thing you should keep in mind is that it does not have domain knowledge, which plays an critical role in model building.\n",
        "\n",
        "         import featuretools as ft\n",
        "\n",
        "  Separate independet and dependet variables (you know how to do). then:\n",
        "\n",
        "         ft.EntitySet(id = 'name')\n",
        "\n",
        "  You can see more in: [FeatureTools](https://featuretools.alteryx.com/en/stable/) "
      ],
      "metadata": {
        "id": "BVzqDV_fhzIq"
      }
    }
  ]
}